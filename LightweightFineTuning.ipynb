{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c05ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install requirements packages:\n",
    "# pip install 'transformers[torch]'\n",
    "## or\n",
    "# pip install transformers\n",
    "\n",
    "# pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbaf7af",
   "metadata": {},
   "source": [
    "## 1. Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46716865",
   "metadata": {},
   "source": [
    "### 1.1 Load a pretrained HF model\n",
    "\n",
    "The code includes the relevant imports and loads a pretrained Hugging Face model designed for sequence classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb568b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████| 483/483 [00:00<00:00, 1.02MB/s]\n",
      "model.safetensors: 100%|██████████| 268M/268M [00:01<00:00, 205MB/s]  \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1},\n",
    ")\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3b82a",
   "metadata": {},
   "source": [
    "### 1.2 Load and preprocess a dataset\n",
    "\n",
    "The code includes the relevant imports and loads a Hugging Face dataset suitable for sequence classification tasks. It then proceeds to include the necessary imports for and loads a Hugging Face tokenizer, which is used to prepare the dataset for processing. To minimize the computational resources required, a subset of the full dataset may be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20cf402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required version of datasets in case you have an older version\n",
    "# You will need to choose \"Kernel > Restart Kernel\" from the menu after executing this cell\n",
    "# ! pip install -q \"datasets==2.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8149422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 7.81k/7.81k [00:00<00:00, 6.77MB/s]\n",
      "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  20%|██        | 4.19M/20.5M [00:00<00:00, 29.6MB/s]\u001b[A\n",
      "Downloading data:  61%|██████▏   | 12.6M/20.5M [00:00<00:00, 46.7MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 33.7MB/s]\u001b[A\n",
      "Downloading data files:  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s]\n",
      "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  20%|█▉        | 4.19M/21.0M [00:00<00:00, 29.3MB/s]\u001b[A\n",
      "Downloading data:  60%|█████▉    | 12.6M/21.0M [00:00<00:00, 46.1MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 38.0MB/s]\u001b[A\n",
      "Downloading data files:  67%|██████▋   | 2/3 [00:01<00:00,  1.70it/s]\n",
      "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  10%|▉         | 4.19M/42.0M [00:00<00:01, 31.4MB/s]\u001b[A\n",
      "Downloading data:  30%|██▉       | 12.6M/42.0M [00:00<00:00, 47.8MB/s]\u001b[A\n",
      "Downloading data:  50%|████▉     | 21.0M/42.0M [00:00<00:00, 54.3MB/s]\u001b[A\n",
      "Downloading data:  70%|██████▉   | 29.4M/42.0M [00:00<00:00, 54.0MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:00<00:00, 55.6MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.53it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1312.36it/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 103742.68 examples/s]\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 102278.84 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 116080.37 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize a new dictionary to hold the modified dataset\n",
    "dataset = {}\n",
    "\n",
    "# Define the splits\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# Load, shuffle, and select a subset for each split\n",
    "for split in splits:\n",
    "    # Load the dataset split\n",
    "    ds = load_dataset(\"imdb\", split=split)\n",
    "\n",
    "    # Shuffle and select the first 500 samples\n",
    "    dataset[split] = ds.shuffle(seed=23).select(range(500))\n",
    "\n",
    "# Display the modified datasets\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 169kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 6.61MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 8.95MB/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 747.07 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 768.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
    "    outputs  = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    return outputs \n",
    "\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f96533d",
   "metadata": {},
   "source": [
    "### 1.3 Evaluate the pretrained model\n",
    "\n",
    "At least one classification metric is calculated by applying the pretrained model to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dcaee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the parameters of the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd86012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.668613</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.659988</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=0.6712383088611421, metrics={'train_runtime': 35.3615, 'train_samples_per_second': 28.279, 'train_steps_per_second': 3.563, 'total_flos': 132467398656000.0, 'train_loss': 0.6712383088611421, 'epoch': 2.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "# My compute_metrics function calculates the \"accuracy\" as a classification metric. \n",
    "# Accuracy is a common metric used to measure the correctness of a model's predictions. \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/positive_negative\",\n",
    "        learning_rate=5e-5,  # Set the learning rate\n",
    "         per_device_train_batch_size=8,  # Set the per device train batch size\n",
    "        per_device_eval_batch_size=16,  # Set the per device eval batch size\n",
    "        evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "        save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db77bb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.65998774766922,\n",
       " 'eval_accuracy': 0.766,\n",
       " 'eval_runtime': 7.9415,\n",
       " 'eval_samples_per_second': 62.96,\n",
       " 'eval_steps_per_second': 4.029,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the performance of the model on the test set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all the parameters of the base model.--For the purpose of comparison\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8024a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 01:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.641921</td>\n",
       "      <td>0.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636097</td>\n",
       "      <td>0.764000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=0.6371427869039868, metrics={'train_runtime': 65.3948, 'train_samples_per_second': 15.292, 'train_steps_per_second': 1.927, 'total_flos': 132467398656000.0, 'train_loss': 0.6371427869039868, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dea75632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6360965371131897,\n",
       " 'eval_accuracy': 0.764,\n",
       " 'eval_runtime': 8.8896,\n",
       " 'eval_samples_per_second': 56.246,\n",
       " 'eval_steps_per_second': 3.6,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the performance of the model on the test set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## 2. Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c96093",
   "metadata": {},
   "source": [
    "### 2.1 Create a PEFT model\n",
    "\n",
    "The code includes the relevant imports, initializes a Hugging Face Parameter-Efficient Fine-Tuning (PEFT) config, and creates a PEFT model using that config."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67bdb4",
   "metadata": {},
   "source": [
    "#### Creating a PEFT Config\n",
    "\n",
    "The PEFT config specifies the adapter configuration for your parameter-efficient fine-tuning process. The base class for this is a PeftConfig, but this example will use a LoraConfig, the subclass used for low rank adaptation (LoRA).\n",
    "\n",
    "A LoRA config can be instantiated like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "config = LoraConfig(target_modules=[\"classifier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b819e",
   "metadata": {},
   "source": [
    "#### Converting a Transformers Model into a PEFT Model\n",
    "\n",
    "Once you have a PEFT config object, you can load a Hugging Face transformers model as a PEFT model by first loading the pre-trained model as usual (here we load GPT-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1}\n",
    ")\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a5af3",
   "metadata": {},
   "source": [
    "Then using get_peft_model() to get a trainable PEFT model (using the LoRA config instantiated previously):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37421833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,160 || all params: 66,961,170 || trainable%: 0.009199361361218747\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "peft_model = get_peft_model(model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47c02cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,196,580 || all params: 67,559,460 || trainable%: 1.7711509239416656\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "peft_model.save_pretrained(\"peft_pretrained\")\n",
    "peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"peft_pretrained\")\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764b96b",
   "metadata": {},
   "source": [
    "####  Load the `IMDB` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8d0cfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 1000\n",
       " })}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize a new dictionary to hold the modified dataset\n",
    "dataset = {}\n",
    "\n",
    "# Define the splits\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# Load, shuffle, and select a subset for each split\n",
    "for split in splits:\n",
    "    # Load the dataset split\n",
    "    ds = load_dataset(\"imdb\", split=split)\n",
    "\n",
    "    # Shuffle and select the first 500 samples\n",
    "    dataset[split] = ds.shuffle(seed=23).select(range(1000))\n",
    "\n",
    "# Display the modified datasets\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e582860",
   "metadata": {},
   "source": [
    "#### Preprocess(tokenize) the `IMDB` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bc13766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:01<00:00, 715.35 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:01<00:00, 750.70 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 1000\n",
       " })}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
    "    outputs  = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    return outputs \n",
    "\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb4663",
   "metadata": {},
   "source": [
    "### 2.2 Train the PEFT model\n",
    "\n",
    "The model undergoes training for at least one epoch, utilizing the Parameter-Efficient Fine-Tuning (PEFT) model and the specified dataset.\n",
    "\n",
    "After calling `get_peft_model()`, you can then use the resulting lora_model in a training process of your choice (PyTorch training loop or Hugging Face Trainer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d31f1",
   "metadata": {},
   "source": [
    "#### Checking Trainable Parameters of a PEFT Model\n",
    "A helpful way to check the number of trainable parameters with the current config is the print_trainable_parameters() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c385f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,196,580 || all params: 67,559,460 || trainable%: 1.7711509239416656\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5818dcb",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/peft/quicktour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc567d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.552454</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.395143</td>\n",
       "      <td>0.816000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.48890200805664064, metrics={'train_runtime': 69.041, 'train_samples_per_second': 28.968, 'train_steps_per_second': 3.621, 'total_flos': 268648538112000.0, 'train_loss': 0.48890200805664064, 'epoch': 2.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa85cd7",
   "metadata": {},
   "source": [
    "### 2.3 Save the PEFT model\n",
    "\n",
    "The fine-tuned parameters of the model are saved to a separate directory, which is located in the same home directory as the notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35945246",
   "metadata": {},
   "source": [
    "#### Saving a Trained PEFT Model\n",
    "Once a PEFT model has been trained, the standard Hugging Face save_pretrained() method can be used to save the weights locally. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"peft_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## 3. Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99725b46",
   "metadata": {},
   "source": [
    "### 3.1 Load the saved PEFT model\n",
    "\n",
    "Includes the relevant imports then loads the saved PEFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5857c01",
   "metadata": {},
   "source": [
    "Because we have only saved the adapter weights and not the full model weights, we can't use from_pretrained() with the regular Transformers class (e.g., AutoModelForCausalLM). Instead, we need to use the PEFT version (e.g., AutoPeftModelForCausalLM). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81423fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"peft_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d9a58a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3951427638530731,\n",
       " 'eval_accuracy': 0.816,\n",
       " 'eval_runtime': 16.6003,\n",
       " 'eval_samples_per_second': 60.24,\n",
       " 'eval_steps_per_second': 7.53,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d6c64",
   "metadata": {},
   "source": [
    "## 4. A Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9133a0b7",
   "metadata": {},
   "source": [
    "||original model (Freeze Params)|original model (tuning Params)|PEFT model|\n",
    "|---|--|--|--|\n",
    "|accuracy|0.766|0.764|0.816|\n",
    "|training time (epoch=2)| 35.36|65.39 |69.04 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2264882e",
   "metadata": {},
   "source": [
    "This table compares the performance and training time of three different setups of a machine learning model: an original model with frozen parameters, the same original model with tunable parameters, and a PEFT (Progressive Embedding Fine-Tuning) model. Each model was evaluated based on its accuracy and the time taken for training over two epochs.\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "* Original Model (Freeze Params): This model achieved an accuracy of 0.766. Here, \"Freeze Params\" indicates that the parameters (weights) of the model were kept constant during training, i.e., they were not updated or changed.\n",
    "\n",
    "* Original Model (Tuning Params): This version of the model, with tunable parameters, achieved a slightly lower accuracy of 0.764. \"Tuning Params\" means that the model's parameters were allowed to update and change during the training process.\n",
    "\n",
    "* PEFT Model: The PEFT model outperformed the other two with an accuracy of 0.816. PEFT typically involves more sophisticated training techniques, often leading to better performance.\n",
    "\n",
    "Training Time (epoch=2):\n",
    "\n",
    "* Original Model (Freeze Params): It took 35.36 units of time (presumably minutes or seconds) for training over 2 epochs. Freezing parameters generally results in shorter training times as fewer calculations are required.\n",
    "\n",
    "* Original Model (Tuning Params): The training time increased to 65.39 units when the parameters were tunable, as this requires more computations to update the weights during training.\n",
    "\n",
    "* PEFT Model: This model had the longest training time at 69.04 units, which is expected due to the more complex nature of the PEFT approach, involving progressive updates to the embedding layers and potentially other parts of the model.\n",
    "\n",
    "In summary, the PEFT model shows the highest accuracy, the training time for the PEFT model and the original model with tunable parameters is quite similar. This makes the PEFT model an efficient choice in terms of balancing performance with computational resources, especially considering the notable improvement in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea12dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c097d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
