{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed57882",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2b2f4",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1fe1fd",
   "metadata": {},
   "source": [
    "## 1. Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f61eff5",
   "metadata": {},
   "source": [
    "### 1.1 Load a pretrained HF model\n",
    "\n",
    "The code includes the relevant imports and loads a pretrained Hugging Face model designed for sequence classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9787b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yychiang\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\yychiang\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1},\n",
    ")\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58332442",
   "metadata": {},
   "source": [
    "### 1.2 Load and preprocess a dataset\n",
    "\n",
    "The code includes the relevant imports and loads a Hugging Face dataset suitable for sequence classification tasks. It then proceeds to include the necessary imports for and loads a Hugging Face tokenizer, which is used to prepare the dataset for processing. To minimize the computational resources required, a subset of the full dataset may be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c177700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required version of datasets in case you have an older version\n",
    "# You will need to choose \"Kernel > Restart Kernel\" from the menu after executing this cell\n",
    "# ! pip install -q \"datasets==2.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3508d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize a new dictionary to hold the modified dataset\n",
    "dataset = {}\n",
    "\n",
    "# Define the splits\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# Load, shuffle, and select a subset for each split\n",
    "for split in splits:\n",
    "    # Load the dataset split\n",
    "    ds = load_dataset(\"imdb\", split=split)\n",
    "\n",
    "    # Shuffle and select the first 500 samples\n",
    "    dataset[split] = ds.shuffle(seed=23).select(range(500))\n",
    "\n",
    "# Display the modified datasets\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d84980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
    "    outputs  = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    return outputs \n",
    "\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9930d3c",
   "metadata": {},
   "source": [
    "### 1.3 Evaluate the pretrained model\n",
    "\n",
    "At least one classification metric is calculated by applying the pretrained model to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a503ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the parameters of the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58c5d941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 07:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.667010</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.659756</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=0.6694205147879464, metrics={'train_runtime': 461.9193, 'train_samples_per_second': 2.165, 'train_steps_per_second': 0.273, 'total_flos': 132467398656000.0, 'train_loss': 0.6694205147879464, 'epoch': 2.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "# My compute_metrics function calculates the \"accuracy\" as a classification metric. \n",
    "# Accuracy is a common metric used to measure the correctness of a model's predictions. \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/positive_negative\",\n",
    "        learning_rate=5e-5,  # Set the learning rate\n",
    "         per_device_train_batch_size=8,  # Set the per device train batch size\n",
    "        per_device_eval_batch_size=16,  # Set the per device eval batch size\n",
    "        evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "        save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6692cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 08:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6597560048103333,\n",
       " 'eval_accuracy': 0.75,\n",
       " 'eval_runtime': 112.076,\n",
       " 'eval_samples_per_second': 4.461,\n",
       " 'eval_steps_per_second': 0.286,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the performance of the model on the test set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2470fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all the parameters of the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b214819d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 12:38, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.642288</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.637578</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=0.6418577527242993, metrics={'train_runtime': 763.153, 'train_samples_per_second': 1.31, 'train_steps_per_second': 0.165, 'total_flos': 132467398656000.0, 'train_loss': 0.6418577527242993, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19bcd5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 01:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6375781893730164,\n",
       " 'eval_accuracy': 0.74,\n",
       " 'eval_runtime': 107.4887,\n",
       " 'eval_samples_per_second': 4.652,\n",
       " 'eval_steps_per_second': 0.298,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the performance of the model on the test set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea48f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "742629c7",
   "metadata": {},
   "source": [
    "## 2. Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9969ec",
   "metadata": {},
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea79b3f",
   "metadata": {},
   "source": [
    "### 2.1 Create a PEFT model\n",
    "\n",
    "The code includes the relevant imports, initializes a Hugging Face Parameter-Efficient Fine-Tuning (PEFT) config, and creates a PEFT model using that config."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c8170",
   "metadata": {},
   "source": [
    "#### Creating a PEFT Config\n",
    "\n",
    "The PEFT config specifies the adapter configuration for your parameter-efficient fine-tuning process. The base class for this is a PeftConfig, but this example will use a LoraConfig, the subclass used for low rank adaptation (LoRA).\n",
    "\n",
    "A LoRA config can be instantiated like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c6f460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "config = LoraConfig(target_modules=[\"classifier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e1d87",
   "metadata": {},
   "source": [
    "#### Converting a Transformers Model into a PEFT Model\n",
    "\n",
    "Once you have a PEFT config object, you can load a Hugging Face transformers model as a PEFT model by first loading the pre-trained model as usual (here we load GPT-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b2a12d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1}\n",
    ")\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d197c",
   "metadata": {},
   "source": [
    "Then using get_peft_model() to get a trainable PEFT model (using the LoRA config instantiated previously):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e34e3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,160 || all params: 66,961,170 || trainable%: 0.009199361361218747\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "peft_model = get_peft_model(model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b53128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 598,290 || all params: 67,559,460 || trainable%: 0.8855754619708328\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "peft_model.save_pretrained(\"peft_pretrained\")\n",
    "peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"peft_pretrained\")\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd436007",
   "metadata": {},
   "source": [
    "####  Load the `IMDB` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b10b0ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize a new dictionary to hold the modified dataset\n",
    "dataset = {}\n",
    "\n",
    "# Define the splits\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# Load, shuffle, and select a subset for each split\n",
    "for split in splits:\n",
    "    # Load the dataset split\n",
    "    ds = load_dataset(\"imdb\", split=split)\n",
    "\n",
    "    # Shuffle and select the first 500 samples\n",
    "    dataset[split] = ds.shuffle(seed=23).select(range(500))\n",
    "\n",
    "# Display the modified datasets\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827930b",
   "metadata": {},
   "source": [
    "#### Preprocess(tokenize) the `IMDB` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "591f0bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
    "    outputs  = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    return outputs \n",
    "\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d9a81",
   "metadata": {},
   "source": [
    "### 2.2 Train the PEFT model\n",
    "\n",
    "The model undergoes training for at least one epoch, utilizing the Parameter-Efficient Fine-Tuning (PEFT) model and the specified dataset.\n",
    "\n",
    "After calling `get_peft_model()`, you can then use the resulting lora_model in a training process of your choice (PyTorch training loop or Hugging Face Trainer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e45dd",
   "metadata": {},
   "source": [
    "#### Checking Trainable Parameters of a PEFT Model\n",
    "A helpful way to check the number of trainable parameters with the current config is the print_trainable_parameters() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad086fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 598,290 || all params: 67,559,460 || trainable%: 0.8855754619708328\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0607f07c",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/peft/quicktour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ca255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 08:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.531912</td>\n",
       "      <td>0.732000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.459229</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=0.5362108018663194, metrics={'train_runtime': 491.1082, 'train_samples_per_second': 2.036, 'train_steps_per_second': 0.257, 'total_flos': 134324269056000.0, 'train_loss': 0.5362108018663194, 'epoch': 2.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d4bb62",
   "metadata": {},
   "source": [
    "### 2.3 Save the PEFT model\n",
    "\n",
    "The fine-tuned parameters of the model are saved to a separate directory, which is located in the same home directory as the notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac41c2",
   "metadata": {},
   "source": [
    "#### Saving a Trained PEFT Model\n",
    "Once a PEFT model has been trained, the standard Hugging Face save_pretrained() method can be used to save the weights locally. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80035a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"peft_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ffb41",
   "metadata": {},
   "source": [
    "## 3. Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c9e2f",
   "metadata": {},
   "source": [
    "### 3.1 Load the saved PEFT model\n",
    "\n",
    "Includes the relevant imports then loads the saved PEFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191508b2",
   "metadata": {},
   "source": [
    "Because we have only saved the adapter weights and not the full model weights, we can't use from_pretrained() with the regular Transformers class (e.g., AutoModelForCausalLM). Instead, we need to use the PEFT version (e.g., AutoPeftModelForCausalLM). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "294a798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"peft_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "466d10c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 01:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.45922911167144775,\n",
       " 'eval_accuracy': 0.784,\n",
       " 'eval_runtime': 109.1066,\n",
       " 'eval_samples_per_second': 4.583,\n",
       " 'eval_steps_per_second': 0.293,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb5468",
   "metadata": {},
   "source": [
    "#### A Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8797e0",
   "metadata": {},
   "source": [
    "||original model (Freeze Params)|original model (tuning Params)|PEFT model|\n",
    "|---|--|--|--|\n",
    "|accuracy|0.75|0.74|0.784|\n",
    "|training time (epoch=2)| 461.9193|763.153 |491.1082 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c06b1d6",
   "metadata": {},
   "source": [
    "This table compares the performance of three different models: the original model with frozen parameters, the original model with tuned parameters, and the PEFT (Progressive Early Termination Fine-Tuning) model.\n",
    "\n",
    "* The \"accuracy\" column represents the accuracy of each model. Accuracy is a measure of how well the model predicts the correct output. The higher the accuracy, the better the model performs. In this case, the original model with frozen parameters has an accuracy of 0.75, the original model with tuned parameters has an accuracy of 0.74, and the PEFT model has an accuracy of 0.784.\n",
    "\n",
    "* The \"training time (epoch=2)\" column represents the time it takes to train each model for 2 epochs. Training time is the time it takes for the model to learn from the training data and adjust its parameters. In this case, the original model with frozen parameters takes 461.9193 seconds to train for 2 epochs, the original model with tuned parameters takes 763.153 seconds, and the PEFT model takes 491.1082 seconds.\n",
    "\n",
    "From this table, we can see that the PEFT model has the highest accuracy among the three models, indicating that it performs the best in terms of prediction accuracy. Additionally, the PEFT model has a shorter training time compared to the original model with tuned parameters, suggesting that it is more efficient in terms of training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8aec06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23374871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
